{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d173a56",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required dependencies for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb26b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy>=1.24.0 pandas>=2.0.0 matplotlib>=3.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731e529",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "Read the sales data from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36683b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7fb1a429-29f0-4fcb-b7cc-eaf09bb7e482\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>3950.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>3844.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>2888.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>3631.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>3072.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fb1a429-29f0-4fcb-b7cc-eaf09bb7e482')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7fb1a429-29f0-4fcb-b7cc-eaf09bb7e482 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7fb1a429-29f0-4fcb-b7cc-eaf09bb7e482');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         date store_id cat_id    sales\n",
       "0  2011-01-29     TX_1  FOODS  3950.35\n",
       "1  2011-01-30     TX_1  FOODS  3844.97\n",
       "2  2011-01-31     TX_1  FOODS  2888.03\n",
       "3  2011-02-01     TX_1  FOODS  3631.28\n",
       "4  2011-02-02     TX_1  FOODS  3072.18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download the sales data directly from GitHub (data branch)\n",
    "url = 'https://raw.githubusercontent.com/jcanizalez/pytorch-time-series-forecasting/main/data/sales.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Preview the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073d480",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "Verify the number of unique stores and product categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c06d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stores: 10\n",
      "Unique categories: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique stores: {df['store_id'].nunique()}\")\n",
    "print(f\"Unique categories: {df['cat_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1799fa",
   "metadata": {},
   "source": [
    "## 3. Date Range Analysis\n",
    "\n",
    "Convert the date column to datetime format and verify the date range of the sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc738f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2011-01-29 00:00:00 to 2016-05-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897dc10",
   "metadata": {},
   "source": [
    "## 4. Check for Duplicates\n",
    "\n",
    "Identify any duplicated date entries within each store and category combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdddbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated date entries: 0\n"
     ]
    }
   ],
   "source": [
    "duplicated_mask = df.duplicated(subset=['store_id', 'cat_id', 'date'], keep=False)\n",
    "num_duplicates = duplicated_mask.sum()\n",
    "print(f\"Number of duplicated date entries: {num_duplicates}\")\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\nSample duplicates:\")\n",
    "    print(df[duplicated_mask].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b412b",
   "metadata": {},
   "source": [
    "## 5. Check for Missing Dates\n",
    "\n",
    "Verify if there are any missing dates in the time series for each store-category combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fa5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected records (complete date range): 58230\n",
      "Actual records: 58230\n",
      "Missing records: 0\n",
      "\n",
      "✓ No missing dates found!\n"
     ]
    }
   ],
   "source": [
    "date_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')\n",
    "expected_records = len(date_range) * df['store_id'].nunique() * df['cat_id'].nunique()\n",
    "actual_records = len(df)\n",
    "missing_records = expected_records - actual_records\n",
    "\n",
    "print(f\"Expected records (complete date range): {expected_records}\")\n",
    "print(f\"Actual records: {actual_records}\")\n",
    "print(f\"Missing records: {missing_records}\")\n",
    "\n",
    "# Check for missing dates per store-category combination\n",
    "if missing_records > 0:\n",
    "    for store in df['store_id'].unique():\n",
    "        for cat in df['cat_id'].unique():\n",
    "            subset = df[(df['store_id'] == store) & (df['cat_id'] == cat)]\n",
    "            subset_dates = set(subset['date'])\n",
    "            all_dates = set(date_range)\n",
    "            missing_dates = all_dates - subset_dates\n",
    "            if len(missing_dates) > 0:\n",
    "                print(f\"\\n{store} - {cat}: {len(missing_dates)} missing dates\")\n",
    "                print(f\"Sample missing dates: {sorted(missing_dates)[:5]}\")\n",
    "                break\n",
    "        if len(missing_dates) > 0:\n",
    "            break\n",
    "else:\n",
    "    print(\"\\n✓ No missing dates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac04a82",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection\n",
    "\n",
    "Identify outliers in the sales data using the Interquartile Range (IQR) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1df67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 (25th percentile): $1342.25\n",
      "Q3 (75th percentile): $4641.09\n",
      "IQR: $3298.85\n",
      "Lower bound: $-3606.02\n",
      "Upper bound: $9589.36\n",
      "\n",
      "Number of outliers: 1455 (2.50%)\n",
      "\n",
      "Sample outliers:\n",
      "           date store_id cat_id     sales\n",
      "1598 2015-06-15     TX_1  FOODS  10036.83\n",
      "6195 2012-02-05     TX_2  FOODS   9683.29\n",
      "6251 2012-04-01     TX_2  FOODS   9969.98\n",
      "6257 2012-04-07     TX_2  FOODS  10122.12\n",
      "6265 2012-04-15     TX_2  FOODS   9692.91\n",
      "6286 2012-05-06     TX_2  FOODS  10007.18\n",
      "6748 2013-08-11     TX_2  FOODS   9699.05\n",
      "6776 2013-09-08     TX_2  FOODS   9604.35\n",
      "6783 2013-09-15     TX_2  FOODS  10379.24\n",
      "6804 2013-10-06     TX_2  FOODS   9649.17\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['sales'].quantile(0.25)\n",
    "Q3 = df['sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['sales'] < lower_bound) | (df['sales'] > upper_bound)]\n",
    "\n",
    "print(f\"Q1 (25th percentile): ${Q1:.2f}\")\n",
    "print(f\"Q3 (75th percentile): ${Q3:.2f}\")\n",
    "print(f\"IQR: ${IQR:.2f}\")\n",
    "print(f\"Lower bound: ${lower_bound:.2f}\")\n",
    "print(f\"Upper bound: ${upper_bound:.2f}\")\n",
    "print(f\"\\nNumber of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\nSample outliers:\")\n",
    "    print(outliers[['date', 'store_id', 'cat_id', 'sales']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5d1d7",
   "metadata": {},
   "source": [
    "## 7. Check for Missing Values\n",
    "\n",
    "Identify any missing values in the sales column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sales values: 0\n",
      "✓ No missing sales values!\n"
     ]
    }
   ],
   "source": [
    "missing_sales = df['sales'].isna().sum()\n",
    "print(f\"Missing sales values: {missing_sales}\")\n",
    "\n",
    "if missing_sales > 0:\n",
    "    print(f\"Percentage of missing values: {missing_sales/len(df)*100:.2f}%\")\n",
    "    print(\"\\nSample records with missing sales:\")\n",
    "    print(df[df['sales'].isna()][['date', 'store_id', 'cat_id', 'sales']].head(10))\n",
    "else:\n",
    "    print(\"✓ No missing sales values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcdafb",
   "metadata": {},
   "source": [
    "## 8. Data Cleaning\n",
    "\n",
    "Replace outliers with null values and impute missing data using linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b7859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total null values (outliers + original missing): 1455\n",
      "Null values after interpolation: 0\n",
      "Successfully imputed: 1455 values\n",
      "\n",
      "Sample of cleaned data:\n",
      "            date store_id cat_id    sales\n",
      "17469 2011-01-29     CA_1  FOODS  7240.65\n",
      "17470 2011-01-30     CA_1  FOODS  6705.51\n",
      "17471 2011-01-31     CA_1  FOODS  4584.85\n",
      "17472 2011-02-01     CA_1  FOODS  4965.46\n",
      "17473 2011-02-02     CA_1  FOODS  4368.07\n",
      "17474 2011-02-03     CA_1  FOODS  5247.42\n",
      "17475 2011-02-04     CA_1  FOODS  5820.45\n",
      "17476 2011-02-05     CA_1  FOODS  8551.46\n",
      "17477 2011-02-06     CA_1  FOODS  7435.86\n",
      "17478 2011-02-07     CA_1  FOODS  5535.84\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "# Set outliers to null\n",
    "df_cleaned.loc[(df_cleaned['sales'] < lower_bound) | (df_cleaned['sales'] > upper_bound), 'sales'] = None\n",
    "\n",
    "# Count nulls after setting outliers to null\n",
    "nulls_before_imputation = df_cleaned['sales'].isna().sum()\n",
    "print(f\"Total null values (outliers + original missing): {nulls_before_imputation}\")\n",
    "\n",
    "# Sort by store, category, and date to ensure proper interpolation\n",
    "df_cleaned = df_cleaned.sort_values(['store_id', 'cat_id', 'date'])\n",
    "\n",
    "# Interpolate missing values within each store-category group\n",
    "df_cleaned['sales'] = df_cleaned.groupby(['store_id', 'cat_id'])['sales'].transform(\n",
    "    lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    ")\n",
    "\n",
    "# Check remaining nulls after interpolation\n",
    "nulls_after_imputation = df_cleaned['sales'].isna().sum()\n",
    "print(f\"Null values after interpolation: {nulls_after_imputation}\")\n",
    "print(f\"Successfully imputed: {nulls_before_imputation - nulls_after_imputation} values\")\n",
    "\n",
    "# Display sample of imputed data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df_cleaned[['date', 'store_id', 'cat_id', 'sales']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ca73c",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data\n",
    "\n",
    "Save the cleaned and preprocessed data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420050cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed data saved to ../data/sales_processed.csv\n",
      "Total records saved: 58230\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "output_path = '../data/sales_processed.csv'\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"✓ Preprocessed data saved to {output_path}\")\n",
    "print(f\"Total records saved: {len(df_cleaned)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
